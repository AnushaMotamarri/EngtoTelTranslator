{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "lines= pd.read_table('uniquew.txt', names=['eng', 'tel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2232, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.eng=lines.eng.str.lower()#apply(lambda x: x.lower())\n",
    "lines.tel=lines.tel.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', str(x)))\n",
    "lines.tel=lines.tel.apply(lambda x: re.sub(\"'\", '', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.tel=lines.tel.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              ­ఊహించలేరు\n",
       "1                   అంచనా\n",
       "2                  అంటాడు\n",
       "3                  అంటారు\n",
       "4                 అంటున్న\n",
       "5                    అంటూ\n",
       "6                    అంటే\n",
       "7                  అంటేనే\n",
       "8                 అంటోంది\n",
       "9                   అండగా\n",
       "10                  అండ్‌\n",
       "11                  అంతకు\n",
       "12                  అంతగా\n",
       "13                అంతర్గత\n",
       "14             అంతర్గతంగా\n",
       "15                   అంతా\n",
       "16               అంతులేని\n",
       "17                   అంతే\n",
       "18                 అందమైన\n",
       "19                అందరికీ\n",
       "20                  అందరూ\n",
       "21                  అందాక\n",
       "22                 అందాలు\n",
       "23                   అంది\n",
       "24                అందినంత\n",
       "25              అందుకుంది\n",
       "26           అకస్మాత్తుగా\n",
       "27                  అక్కడ\n",
       "28               అక్కడికీ\n",
       "29                 అక్కడే\n",
       "              ...        \n",
       "2202             స్థాయిని\n",
       "2203             స్థితిలో\n",
       "2204               స్నేహం\n",
       "2205           స్నేహితులు\n",
       "2206             స్పందనకు\n",
       "2207        స్పర్శనిబట్టి\n",
       "2208                స్పృహ\n",
       "2209           స్ఫూర్తిని\n",
       "2210    స్ఫూర్తిప్రదాతలని\n",
       "2211             స్రావాలు\n",
       "2212                స్వంత\n",
       "2213          స్వభావానికి\n",
       "2214                స్వయం\n",
       "2215              స్వరంతో\n",
       "2216              స్వార్థ\n",
       "2217         స్వీకరించండి\n",
       "2218              స్వేచ్ఛ\n",
       "2219            స్వేచ్ఛతో\n",
       "2220              హమ్మయ్య\n",
       "2221             హార్మోన్\n",
       "2222               హుషారు\n",
       "2223                హృదయం\n",
       "2224         హెచ్చరించారో\n",
       "2225            హెచ్‌సీయూ\n",
       "2226               హైపర్‌\n",
       "2227                 హైవే\n",
       "2228                హోటల్\n",
       "2229              హోటల్లో\n",
       "2230                 హోదా\n",
       "2231           హోరుకిహోరు\n",
       "Name: eng, Length: 2232, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
    "lines.tel = lines.tel.apply(lambda x: re.sub(\"[0123456789]\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.eng=lines.eng.apply(lambda x: x.strip())\n",
    "lines.tel=lines.tel.apply(lambda x: x.strip())\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines.tel=lines.tel.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "lines.tel = lines.tel.apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>tel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>కౌన్సిలింగ్‌</td>\n",
       "      <td>START_ కౌన్సిల్‌ _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>ఇటు</td>\n",
       "      <td>START_ ఇటు _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>పోనిచ్చాడు</td>\n",
       "      <td>START_ పోయి ఇచ్చు _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>జరిపి</td>\n",
       "      <td>START_ జరగు _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>చేస్తారు</td>\n",
       "      <td>START_ చేయు _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>అవలంబించాలి</td>\n",
       "      <td>START_ అవలంబించు _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>చెల్లెలికన్నా</td>\n",
       "      <td>START_ చెల్లెలు _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>ఎదురయ్యే</td>\n",
       "      <td>START_ ఎదురు అవ్వు _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>పోలికలను</td>\n",
       "      <td>START_ పోలిక _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>పొరపాట్లు</td>\n",
       "      <td>START_ పొరపాటు _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                eng                      tel\n",
       "672    కౌన్సిలింగ్‌    START_ కౌన్సిల్‌ _END\n",
       "262             ఇటు          START_ ఇటు _END\n",
       "1455     పోనిచ్చాడు   START_ పోయి ఇచ్చు _END\n",
       "937           జరిపి         START_ జరగు _END\n",
       "907        చేస్తారు         START_ చేయు _END\n",
       "165     అవలంబించాలి    START_ అవలంబించు _END\n",
       "854   చెల్లెలికన్నా     START_ చెల్లెలు _END\n",
       "401        ఎదురయ్యే  START_ ఎదురు అవ్వు _END\n",
       "1459       పోలికలను        START_ పోలిక _END\n",
       "1449      పొరపాట్లు      START_ పొరపాటు _END"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eng_words=set()\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Telugu_words=set()\n",
    "for tel in lines.tel:\n",
    "    for word in tel.split():\n",
    "        if word not in all_Telugu_words:\n",
    "            all_Telugu_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "lenght_list=[]\n",
    "for l in lines.eng:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(lenght_list)\n",
    "print(max_length_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "lenght_list=[]\n",
    "for l in lines.tel:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(lenght_list)\n",
    "\n",
    "print(max_length_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210 1215\n"
     ]
    }
   ],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_Telugu_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_Telugu_words)\n",
    "print(num_encoder_tokens, num_decoder_tokens)\n",
    "num_decoder_tokens += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              eng                       tel\n",
      "1417   పెట్టుకునే    START_ పెట్టు అను _END\n",
      "1148     దిగులుగా        START_ దిగులు _END\n",
      "1624       మధ్యగల       START_ మధ్య కల _END\n",
      "574         కావడం         START_ కావడం _END\n",
      "634        కొంచెం        START_ కొంచెం _END\n",
      "1733       మునిగి        START_ మునుగు _END\n",
      "1862         లేరు          START_ లేరు _END\n",
      "1666    మలచుకోవడం          START_ మలచు _END\n",
      "2088  శనాదివారాలు  START_ శని ఆది వారం _END\n",
      "1691      మానిక్‌       START_ మానిక్‌ _END\n"
     ]
    }
   ],
   "source": [
    "lines = shuffle(lines)\n",
    "print(lines.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2008,) (224,)\n"
     ]
    }
   ],
   "source": [
    "X, y = lines.eng, lines.tel\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle('Weights_Tel/X_train.pkl')\n",
    "X_test.to_pickle('Weights_Tel/X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 20):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src+1),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar+1),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar+1, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens+1, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2008"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "val_samples\n",
    "train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/20\n",
      "100/100 [==============================] - 15s 152ms/step - loss: 5.3022 - acc: 0.4543 - val_loss: 3.9337 - val_acc: 0.4512\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 3.8246 - acc: 0.4602 - val_loss: 3.9403 - val_acc: 0.4521\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 9s 85ms/step - loss: 3.7734 - acc: 0.4600 - val_loss: 3.9897 - val_acc: 0.4484\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 3.7002 - acc: 0.4601 - val_loss: 4.0459 - val_acc: 0.4503\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 9s 85ms/step - loss: 3.6250 - acc: 0.4598 - val_loss: 4.0968 - val_acc: 0.4503\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 9s 85ms/step - loss: 3.5680 - acc: 0.4601 - val_loss: 4.1458 - val_acc: 0.4512\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 3.5319 - acc: 0.4603 - val_loss: 4.2449 - val_acc: 0.4494\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 3.4999 - acc: 0.4604 - val_loss: 4.2828 - val_acc: 0.4512\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 3.4871 - acc: 0.4612 - val_loss: 4.3315 - val_acc: 0.4530\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 3.4733 - acc: 0.4617 - val_loss: 4.3872 - val_acc: 0.4512\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 9s 88ms/step - loss: 3.4618 - acc: 0.4609 - val_loss: 4.4020 - val_acc: 0.4494\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 3.4485 - acc: 0.4617 - val_loss: 4.3784 - val_acc: 0.4503\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 9s 89ms/step - loss: 3.4394 - acc: 0.4614 - val_loss: 4.4745 - val_acc: 0.4512\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 3.4353 - acc: 0.4622 - val_loss: 4.3811 - val_acc: 0.4584\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 3.4205 - acc: 0.4618 - val_loss: 4.3899 - val_acc: 0.4567\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 3.4139 - acc: 0.4620 - val_loss: 4.3994 - val_acc: 0.4586\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 9s 89ms/step - loss: 3.3997 - acc: 0.4622 - val_loss: 4.4037 - val_acc: 0.4566\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 9s 89ms/step - loss: 3.3974 - acc: 0.4621 - val_loss: 4.4388 - val_acc: 0.4596\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 3.3784 - acc: 0.4626 - val_loss: 4.5156 - val_acc: 0.4577\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 9s 89ms/step - loss: 3.3690 - acc: 0.4622 - val_loss: 4.4674 - val_acc: 0.4596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c1421ec50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = int(train_samples/batch_size),\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = int(val_samples/batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('nmt_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('nmt_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: నేను\n",
      "Actual Tel Translation:  నేను \n",
      "Predicted Tel Translation:  మరి \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Tel Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Tel Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
